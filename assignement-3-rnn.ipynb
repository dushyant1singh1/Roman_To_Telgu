{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8239567,"sourceType":"datasetVersion","datasetId":4887607},{"sourceId":8315756,"sourceType":"datasetVersion","datasetId":4939658}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport torch\nimport os\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nimport random\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T13:38:11.347430Z","iopub.execute_input":"2024-05-15T13:38:11.347986Z","iopub.status.idle":"2024-05-15T13:38:11.354551Z","shell.execute_reply.started":"2024-05-15T13:38:11.347952Z","shell.execute_reply":"2024-05-15T13:38:11.353256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _ ,filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,filename))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:17.685825Z","iopub.execute_input":"2024-05-15T13:38:17.686183Z","iopub.status.idle":"2024-05-15T13:38:17.701326Z","shell.execute_reply.started":"2024-05-15T13:38:17.686155Z","shell.execute_reply":"2024-05-15T13:38:17.700376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:18.124626Z","iopub.execute_input":"2024-05-15T13:38:18.125028Z","iopub.status.idle":"2024-05-15T13:38:18.157902Z","shell.execute_reply.started":"2024-05-15T13:38:18.124998Z","shell.execute_reply":"2024-05-15T13:38:18.156611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Hyperparameters:\n    def __init__(self,input_dim:int,output_dim:int,\n                 encoder_layers =1,decoder_layers=1,hidden_size=64,embed_dim=512,num_layers=1\n                 ,cell_type:str='rnn',bidirectional:bool=False,dropout:float=0,beam_search:int=0,\n                 learning_rate=0.001):\n        self.encoder_layers = encoder_layers\n        self.decoder_layers = decoder_layers\n        self.hidden_size = hidden_size\n        #input_dim is size of vocabulary of input language\n        self.input_dim = input_dim\n        self.embed_dim = embed_dim\n        self.num_layers = num_layers\n        #output_dim is size of vocabulary of output language\n        self.output_dim = output_dim\n    \n        cell_dict = {'rnn':nn.RNN,'gru':nn.GRU,'lstm':nn.LSTM}\n        self.cell = cell_dict[cell_type]\n        self.cell_name = cell_type\n        self.bidirectional = bidirectional\n        self.dropout = dropout\n        self.beam_search = beam_search\n        self.learning_rate = learning_rate","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:20.954422Z","iopub.execute_input":"2024-05-15T13:38:20.954822Z","iopub.status.idle":"2024-05-15T13:38:20.965771Z","shell.execute_reply.started":"2024-05-15T13:38:20.954778Z","shell.execute_reply":"2024-05-15T13:38:20.964905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parameters - this class contains all the configurations for the model\nclass EncoderRNN(nn.Module):\n    def __init__(self,parameters:Hyperparameters):\n        super(EncoderRNN,self).__init__()\n        # hidden_dim - number of the neuron in the hidden state\n        self.hidden_dim = parameters.hidden_size\n        # num_layers - number of the layers in the encoder\n        self.num_layers = parameters.encoder_layers\n        # parameters.embedding - size of the embedding vector\n        # parameters.input_dim - size of the vocabulary dictionary\n        self.embedding = nn.Embedding(parameters.input_dim,parameters.embed_dim,padding_idx = 2)\n        # parameters.cell - the type of cell : RNN, LSTM, GRU\n        self.cell_name = parameters.cell\n        self.dropout = nn.Dropout(parameters.dropout)\n        self.cell = parameters.cell(parameters.embed_dim,parameters.hidden_size,num_layers=self.num_layers,batch_first=True, dropout=parameters.dropout)\n        #batch_first=False, dropout=0.0, bidirectional=False\n    \n    def forward(self,input_data,h_0):\n        \n        embedded = self.embedding(input_data)\n        embedded = self.dropout(embedded)\n        output, hidden = self.cell(embedded,h_0)\n        return output,hidden\n\n    def hidden_initializer(self,batch_size):\n        return torch.zeros(self.num_layers,batch_size,self.hidden_dim,device = device)\n    \nclass DecoderRNN(nn.Module):\n    def __init__(self,parameters:Hyperparameters):\n        super(DecoderRNN,self).__init__()\n        # hidden_dim - number of neurons in the hidden state\n        self.hidden_dim = parameters.hidden_size\n        # num_layers - number of decoder layers\n        self.num_layers = parameters.decoder_layers\n        # cell_name - LSTM, GRU, RNN\n        self.cell_name = parameters.cell\n        self.embedding = nn.Embedding(parameters.output_dim,parameters.embed_dim)\n        self.dropout = nn.Dropout(parameters.dropout)\n        self.cell = parameters.cell(parameters.embed_dim,self.hidden_dim,num_layers=self.num_layers,batch_first=True, dropout=parameters.dropout)\n        self.out = nn.Linear(parameters.hidden_size,parameters.output_dim)\n        self.softmax = nn.LogSoftmax(dim=2)\n        \n    def forward(self,input_data,h_0):\n        embedded = self.embedding(input_data)\n        activation = F.relu(embedded)\n        activation = self.dropout(activation)\n        output, hidden = self.cell(activation, h_0)\n        output = self.softmax(self.out(output))\n        return output,hidden","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:06:24.452830Z","iopub.execute_input":"2024-05-15T14:06:24.454041Z","iopub.status.idle":"2024-05-15T14:06:24.467347Z","shell.execute_reply.started":"2024-05-15T14:06:24.454000Z","shell.execute_reply":"2024-05-15T14:06:24.466285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\nPAD_token = 2\ndef characterFetching(x):\n    characters = 3\n    ind2ch ={SOS_token:'<',EOS_token:'>',PAD_token:'_'}\n    ch2ind ={'<':SOS_token,'>':EOS_token,'_':PAD_token}\n    for word in x:\n        for letter in word:\n            if letter not in ch2ind:\n                ch2ind[letter] = characters\n                ind2ch[characters] = letter\n                characters+=1\n    return [ch2ind,ind2ch,characters]\ndef wordPairs(x,y):\n    return [[x[i],y[i]] for i in range(len(x))]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:24.305542Z","iopub.execute_input":"2024-05-15T13:38:24.306402Z","iopub.status.idle":"2024-05-15T13:38:24.313845Z","shell.execute_reply.started":"2024-05-15T13:38:24.306371Z","shell.execute_reply":"2024-05-15T13:38:24.312639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataLoading(data_type):\n    path = \"/kaggle/input/roman-to-telgu/tel_{}.csv\".format(data_type)\n    df = pd.read_csv(path,header=None)\n    return df[0].to_numpy(), df[1].to_numpy()    ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:26.801396Z","iopub.execute_input":"2024-05-15T13:38:26.802277Z","iopub.status.idle":"2024-05-15T13:38:26.807402Z","shell.execute_reply.started":"2024-05-15T13:38:26.802245Z","shell.execute_reply":"2024-05-15T13:38:26.806151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_data, train_output_data = dataLoading('train')\nval_input_data, val_output_data = dataLoading('valid')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:29.483087Z","iopub.execute_input":"2024-05-15T13:38:29.483899Z","iopub.status.idle":"2024-05-15T13:38:29.658125Z","shell.execute_reply.started":"2024-05-15T13:38:29.483858Z","shell.execute_reply":"2024-05-15T13:38:29.657128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_input_data,train_output_data)\nprint(val_input_data, val_output_data )","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:31.308321Z","iopub.execute_input":"2024-05-15T13:38:31.308737Z","iopub.status.idle":"2024-05-15T13:38:31.314774Z","shell.execute_reply.started":"2024-05-15T13:38:31.308690Z","shell.execute_reply":"2024-05-15T13:38:31.313784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_en, train_hin, valid_en and valid_hin all are list of length 3\n# 0 index contain dictionary for characters to index\n# 1 index contain dictionary for index to characters\n# 2 index contain number of unique characters \n# en - english and hin - hindi\n# train - training data , valid - validation data\ntrain_en = characterFetching(train_input_data)\ntrain_hin = characterFetching(train_output_data)\ntrain_wordpairs = wordPairs(train_input_data,train_output_data)\nvalid_en = characterFetching(val_input_data)\nvalid_hin= characterFetching(val_output_data)\nvalid_wordpairs = wordPairs(val_input_data,val_output_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:32.665022Z","iopub.execute_input":"2024-05-15T13:38:32.665648Z","iopub.status.idle":"2024-05-15T13:38:32.928105Z","shell.execute_reply.started":"2024-05-15T13:38:32.665618Z","shell.execute_reply":"2024-05-15T13:38:32.927067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_en[2])\nprint(train_hin[2])\nprint(valid_en[2])\nprint(valid_hin[2])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:35.091773Z","iopub.execute_input":"2024-05-15T13:38:35.092647Z","iopub.status.idle":"2024-05-15T13:38:35.097669Z","shell.execute_reply.started":"2024-05-15T13:38:35.092614Z","shell.execute_reply":"2024-05-15T13:38:35.096659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#input_t and output_t are I have stored character to index dictionary and one pair is given\ndef mannualPadding(x,padding_index,max_length):\n    length_of_padding = max_length - len(x)\n    padded_list = [padding_index]*(length_of_padding)\n    x.extend(padded_list)\n    return x\ndef gettingTensorFromPair(pair,input_t,output_t,padding_index,max_length):\n    word_en = pair[0]\n    word_hin = pair[1]\n    indexes_en = [input_t[char] for char in word_en]\n    indexes_hin = [output_t[char] for char in word_hin]\n    indexes_en.append(EOS_token)\n    indexes_hin.append(EOS_token)\n    \n    indexes_en = mannualPadding(indexes_en,padding_index,max_length)\n    indexes_hin = mannualPadding(indexes_hin,padding_index,max_length)\n    \n    input_tensor = torch.tensor(indexes_en,dtype=torch.long,device=device)\n    output_tensor = torch.tensor(indexes_hin,dtype=torch.long,device=device)\n    return input_tensor,output_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:43.358852Z","iopub.execute_input":"2024-05-15T13:38:43.359234Z","iopub.status.idle":"2024-05-15T13:38:43.368399Z","shell.execute_reply.started":"2024-05-15T13:38:43.359206Z","shell.execute_reply":"2024-05-15T13:38:43.367276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nBATCH_SIZE = 32\nMAX_LENGTH = 30\ntrain_data = [gettingTensorFromPair(pair,train_en[0],train_hin[0],2,MAX_LENGTH) for pair in train_wordpairs]\nval_data = [gettingTensorFromPair(pair,valid_en[0],valid_hin[0],2,MAX_LENGTH) for pair in valid_wordpairs]\n\n# train_input_tensors , train_output_tensors = [pair[0] for pair in train_data ],[pair[1] for pair in train_data]\n# val_input_tensors, val_output_tensors = [pair[0] for pair in val_data], [pair[1] for pair in val_data]\n\n\n\ntrain_data = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle= True)\nvalid_data = DataLoader(val_data,batch_size = BATCH_SIZE,shuffle = True)\n\n# train_input_loader = DataLoader(train_input_tensors,BATCH_SIZE)\n# train_output_loader = dataLoader(train_output_tensors,BATCH_SIZE)\n# val_input_loader = dataLoader(val_input_tensors,BATCH_SIZE)\n# val_output_loader = dataLoader(val_output_tensors,BATCH_SIZE)\n# valid_data = [val_input_loader,val_output_loader]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:53.838824Z","iopub.execute_input":"2024-05-15T13:38:53.839469Z","iopub.status.idle":"2024-05-15T13:38:57.808576Z","shell.execute_reply.started":"2024-05-15T13:38:53.839436Z","shell.execute_reply":"2024-05-15T13:38:57.807644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for input_t, labels in train_data:\n    print(input_t.size())\n    for word in input_t:\n        print([train_en[1][char.item()] for char in word])\n        break\n    for word in labels:\n        print([train_hin[1][char.item()] for char in word])\n        break\n    print(labels.size())\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:38:57.811900Z","iopub.execute_input":"2024-05-15T13:38:57.812233Z","iopub.status.idle":"2024-05-15T13:38:57.877555Z","shell.execute_reply.started":"2024-05-15T13:38:57.812205Z","shell.execute_reply":"2024-05-15T13:38:57.876564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_string(predicted_sequences,output_tensor,input_tensor,type_of_data):\n    \n    if type_of_data=='train':\n        for i in range(5):\n            predicted_string = \"\"\n            target_string =\"\"\n            input_string =\"\"\n            for j in range(predicted_sequences.size(1)):\n                predicted_string += train_hin[1][predicted_sequences[i,j].item()]\n                target_string += train_hin[1][output_tensor[i,j].item()]\n                input_string += train_en[1][input_tensor[i,j].item()]\n            print(\"{} {} {}\".format(predicted_string,target_string,input_string))\n    else:\n        for i in range(5):\n            predicted_string = \"\"\n            target_string =\"\"\n            input_string =\"\"\n            for j in range(predicted_sequences.size(1)):\n                predicted_string += train_hin[1][predicted_sequences[i,j].item()]\n                target_string += train_hin[1][output_tensor[i,j].item()]\n                input_string += train_en[1][input_tensor[i,j].item()]\n            print(\"{} {} {}\".format(predicted_string,target_string,input_string))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:39:06.476036Z","iopub.execute_input":"2024-05-15T13:39:06.476527Z","iopub.status.idle":"2024-05-15T13:39:06.486890Z","shell.execute_reply.started":"2024-05-15T13:39:06.476487Z","shell.execute_reply":"2024-05-15T13:39:06.485684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(para,encoder,decoder,data,batch_size,type_of_data):\n    encoder.eval()\n    decoder.eval()\n    criterion = nn.NLLLoss()\n    correct_predictions =0 \n    total=0\n    total_loss =0\n    batch_length = len(data)\n    with torch.no_grad():\n        for input_batch , output_batch in data:\n            loss = 0\n            \n            #predicted_string_index = torch.zeros(input_data.size(1),batch_size,1)\n            \n            input_tensor = input_batch.to(device)\n            output_tensor = output_batch.to(device)\n            \n            encoder_hidden = encoder.hidden_initializer(batch_size)\n            if para.cell_name=='lstm':\n                encoder_hidden = (encoder_hidden,encoder.hidden_initializer(batch_size))\n                \n            output_length = output_tensor.size(0)\n            \n            encoder_out , encoder_hidden = encoder(input_tensor,encoder_hidden)\n                \n            decoder_input = torch.full((batch_size,1),SOS_token,device = device)\n            #print(decoder_input.size())\n            #print(output_tensor.size())\n            decoder_hidden = encoder_hidden\n            predicted_sequences = []\n            for j in range(output_batch.size(1)):\n                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n                loss+= criterion(decoder_output[:,-1,:],output_tensor[:,j])\n                # Get predicted tokens\n                _, topi = decoder_output.topk(1)\n                predicted_sequences.append(topi.squeeze().tolist())\n                \n                # Use predicted token as next input\n                decoder_input = topi.squeeze().detach().view(batch_size,1)\n            total_loss += loss.item()/output_tensor.size(1)\n            # Convert predicted sequences to tensors\n            predicted_sequences = torch.transpose(torch.tensor(predicted_sequences),0,1).to(device)\n           # make_string(predicted_sequences,output_tensor,input_tensor,type_of_data)\n            # Compare predicted sequences with target sequences\n            correct_predictions += torch.sum((predicted_sequences == output_tensor).all(dim=1)).item()\n            total += batch_size\n        return correct_predictions/total, total_loss/batch_length","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:39:10.903460Z","iopub.execute_input":"2024-05-15T13:39:10.904159Z","iopub.status.idle":"2024-05-15T13:39:10.916922Z","shell.execute_reply.started":"2024-05-15T13:39:10.904128Z","shell.execute_reply":"2024-05-15T13:39:10.915759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(encoder:EncoderRNN,decoder:DecoderRNN,epochs:int,para:Hyperparameters,train_data,valid_data,batch_size,teacher_forcing_ratio):\n    encoder_opt = optim.Adam(encoder.parameters(),para.learning_rate)\n    decoder_opt = optim.Adam(decoder.parameters(),para.learning_rate)\n    criterion = nn.NLLLoss()\n    \n    for epch in range(epochs):\n        encoder.train()\n        decoder.train()\n        for ind, (input_tensor, output_tensor) in enumerate(tqdm(train_data, desc=f'Training Progress {epch+1}')):\n       # for input_tensor, output_tensor in zip(input_t,output_t):\n            encoder_opt.zero_grad()\n            decoder_opt.zero_grad()\n            \n            input_length = input_tensor.size(0)\n            output_length = output_tensor.size(0)\n            \n            input_tensor = input_tensor.to(device)\n            output_tensor = output_tensor.to(device)\n            # D*num_layers , batch_size, number of neurons in hidden layer\n            encoder_hidden = encoder.hidden_initializer(batch_size)\n            if para.cell_name=='lstm':\n                   encoder_hidden = (encoder_hidden, encoder.hidden_initializer(batch_size))\n            \n            loss =0\n            encoder_out , encoder_hidden = encoder(input_tensor,encoder_hidden)\n                \n            #decoder_input = torch.full((batch_size,1),SOS_token,device = device)\n            decoder_input = output_tensor[:,0].view(batch_size,1)\n            \n            decoder_hidden = encoder_hidden\n\n            teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n#             if teacher_forcing:\n#                 for j in range(output_tensor.size(1)):\n                    \n#                     decoder_out, decoder_hidden = decoder(decoder_input, decoder_hidden)\n#                     loss+= criterion(decoder_out[:,-1,:],output_tensor[:,j])\n#                     decoder_input = output_tensor[:,j].unsqueeze(1)\n#             else:\n#                 for j in range(output_tensor.size(1)):\n#                     decoder_out, decoder_hidden = decoder(decoder_input,decoder_hidden)\n#                     #decoder.size = batch size , sequence length, output vocabulry size\n#                     loss += criterion(decoder_out[:,-1,:], output_tensor[:, j])\n#                     topv, topi = decoder_out.topk(1)\n                    \n#                     decoder_input = topi.squeeze().detach().view(batch_size,1)\n            for j in range(output_tensor.size(1)):\n                decoder_out, decoder_hidden = decoder(decoder_input,decoder_hidden)\n                topv, topi = decoder_out.topk(1)\n                decoder_input = topi.squeeze().detach().view(batch_size,1)\n                loss+=criterion(decoder_out[:,-1,:],output_tensor[:,j])\n                if(j<output_tensor.size(1)-1):\n                    if teacher_forcing:\n                        decoder_input = output_tensor[:,j+1].view(batch_size,1)\n    \n    \n            loss.backward()\n            encoder_opt.step()\n            decoder_opt.step()\n        train_acc, train_loss = accuracy(para,encoder,decoder,train_data,batch_size,'train')\n        val_acc, val_loss = accuracy(para,encoder,decoder,valid_data,batch_size,'valid')\n        print(\"Training accuracy for epoch {} is - {} and loss - {}\".format((epch+1),train_acc,train_loss))\n        print(\"Validation accuracy is - {} and loss -{}\".format(val_acc,val_loss))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:43:25.150823Z","iopub.execute_input":"2024-05-15T13:43:25.151186Z","iopub.status.idle":"2024-05-15T13:43:25.166883Z","shell.execute_reply.started":"2024-05-15T13:43:25.151159Z","shell.execute_reply":"2024-05-15T13:43:25.165885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = Hyperparameters(input_dim=train_en[2],output_dim=train_hin[2],encoder_layers = 15,decoder_layers =15,cell_type='lstm',bidirectional=True,hidden_size = 256)\nencoder = EncoderRNN(parameters).to(device)\ndecoder = DecoderRNN(parameters).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:07:24.699869Z","iopub.execute_input":"2024-05-15T14:07:24.700269Z","iopub.status.idle":"2024-05-15T14:07:24.921735Z","shell.execute_reply.started":"2024-05-15T14:07:24.700244Z","shell.execute_reply":"2024-05-15T14:07:24.920853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(encoder,decoder,20,parameters,train_data,valid_data,batch_size=BATCH_SIZE,teacher_forcing_ratio=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:07:28.017179Z","iopub.execute_input":"2024-05-15T14:07:28.017559Z","iopub.status.idle":"2024-05-15T15:11:25.224963Z","shell.execute_reply.started":"2024-05-15T14:07:28.017530Z","shell.execute_reply":"2024-05-15T15:11:25.223248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nencoder = None\ndecoder = None\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T14:01:04.668602Z","iopub.execute_input":"2024-04-29T14:01:04.669305Z","iopub.status.idle":"2024-04-29T14:01:04.883933Z","shell.execute_reply.started":"2024-04-29T14:01:04.669275Z","shell.execute_reply":"2024-04-29T14:01:04.882708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}